{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GbMy1oL3EIU"
   },
   "source": [
    "##Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ===== Ядро =====\n",
    "\n",
    "class Mult:\n",
    "  \"\"\"Слой перемножения матриц: y = X @ W\"\"\"\n",
    "  def __init__(self, W: np.ndarray):\n",
    "    self.W = W\n",
    "    self.x = None\n",
    "    self.W_grad = None\n",
    "    self.x_grad = None\n",
    "\n",
    "  def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "    self.x = x\n",
    "    return x @ self.W\n",
    "\n",
    "  def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "    self.W_grad = self.x.T @ grad\n",
    "    self.x_grad = grad @ self.W.T\n",
    "    return self.x_grad\n",
    "\n",
    "\n",
    "class AddC:\n",
    "  \"\"\"Добавление константы (смещения): y = X + b\"\"\"\n",
    "  def __init__(self, b: np.ndarray):\n",
    "    self.b = b\n",
    "    self.b_grad = None\n",
    "\n",
    "  def __call__(self,x: np.ndarray) -> np.ndarray:\n",
    "    return x + self.b\n",
    "\n",
    "  def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "    self.b_grad = grad.sum(axis=0)\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Linear:\n",
    "  \"\"\"Линейный слой: y = X @ W + b\"\"\"\n",
    "  def __init__(self, in_features: int, out_features: int):\n",
    "    # инициализация Кайминга\n",
    "    self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
    "    self.b = np.random.rand(out_features)\n",
    "    self.mult = Mult(self.W)\n",
    "    self.add = AddC(self.b)\n",
    "\n",
    "  def __call__(self,  x: np.ndarray) -> np.ndarray:\n",
    "    return self.forward(x)\n",
    "\n",
    "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "    self.x = x\n",
    "    z = self.mult(x)\n",
    "    self.y = self.add(z)\n",
    "    return self.y\n",
    "\n",
    "  def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "    grad = self.add.backward(grad)\n",
    "    grad = self.mult.backward(grad)\n",
    "    self.W_grad = self.mult.W_grad\n",
    "    self.b_grad = self.add.b_grad\n",
    "    return grad\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "  \"\"\"Функция активации ReLU\"\"\"\n",
    "  def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "    self.x = x\n",
    "    return np.clip(x, 0, np.inf)\n",
    "\n",
    "  def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "    self.grad = (self.x > 0).astype(float)\n",
    "    return grad * self.grad\n",
    "\n",
    "\n",
    "class MSE:\n",
    "  \"\"\"Фукнция потерь по формуле MSE\"\"\"\n",
    "  def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    n = y_true.shape[0]\n",
    "    sq_er = (y_pred - y_true) ** 2\n",
    "    self.sum_layer = SumE()\n",
    "    loss = self.sum_layer(sq_er) / n\n",
    "    return loss\n",
    "\n",
    "  def backward(self, y_pred, y_true) -> np.ndarray:\n",
    "    n = y_true.shape[0]\n",
    "    grad = 2 * (y_pred - y_true) / n\n",
    "    return self.sum_layer.backward(grad)\n",
    "\n",
    "class SumE:\n",
    "  \"\"\"Сложение элементов одной матрицы\"\"\"\n",
    "  def __init__(self):\n",
    "    self.shape = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.shape = x.shape\n",
    "    self.y = x.sum()\n",
    "    return self.y\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.forward(x)\n",
    "\n",
    "  def backward(self, grad):\n",
    "    self.grad = np.ones(self.shape)\n",
    "    return self.grad * grad\n",
    "\n",
    "\n",
    "class SumM:\n",
    "  \"\"\"Сложение матриц\"\"\"\n",
    "  def __init__(self):\n",
    "    self.shapes=None\n",
    "\n",
    "  def forward(self,*tensors):\n",
    "    self.shapes=[t.shape for t in tensors]\n",
    "    y = sum(tensors)\n",
    "    return y\n",
    "\n",
    "  def __call__(self,*x):\n",
    "    return self.forward(*x)\n",
    "\n",
    "  def backward(self,grad):\n",
    "    return [grad]*len(self.shapes)\n",
    "\n",
    "\n",
    "class Concat:\n",
    "  \"\"\"Конкатенация тензоров\"\"\"\n",
    "  def __init__(self,axis):\n",
    "    self.shapes=None\n",
    "    self.axis=axis\n",
    "\n",
    "  def forward(self,*tensors):\n",
    "    self.shapes=[t.shape for t in tensors]\n",
    "    y=np.concatenate(tensors,axis=self.axis)\n",
    "    return y\n",
    "\n",
    "  def __call__(self,*tensors):\n",
    "    return self.forward(*tensors)\n",
    "\n",
    "  def backward(self,grad):\n",
    "    grads=[]\n",
    "    start_idx=0\n",
    "    for shape in self.shapes:\n",
    "      end_idx=start_idx+shape[self.axis]\n",
    "      slices=[]\n",
    "      for dim in range(grad.ndim):\n",
    "        if dim==self.axis:\n",
    "          slices.append(slice(start_idx,end_idx))\n",
    "        else:\n",
    "          slices.append(slice(None))\n",
    "      grads.append(grad[tuple(slices)])\n",
    "      start_idx=end_idx\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Оптимизатор =====\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class Optimizer:\n",
    "  def __init__(self, x, y, model, form = 'adamw', loss_fn = MSE(), val=(None, None), lr=1e-3, betas=(0.9, 0.999), wd=1e-2, eps=1e-8):\n",
    "    self.x, self.y = x, y\n",
    "    self.form = form.lower()\n",
    "    self.model = model\n",
    "    self.loss_fn = loss_fn\n",
    "    self.val_x, self.val_y = val\n",
    "    self.lr, self.wd = lr, wd\n",
    "    self.mom1, self.mom2 = betas\n",
    "    self.eps=eps\n",
    "    self.step_count=0\n",
    "\n",
    "    #параметры\n",
    "    self.layers = [l for l in model.layers if isinstance(l, Linear)]\n",
    "\n",
    "    self.m_w = [np.zeros_like(l.W) for l in self.layers]\n",
    "    self.v_w = [np.zeros_like(l.W) for l in self.layers]\n",
    "    self.m_b = [np.zeros_like(l.b) for l in self.layers]\n",
    "    self.v_b = [np.zeros_like(l.b) for l in self.layers]\n",
    "\n",
    "  def step(self):\n",
    "    self.step_count += 1\n",
    "\n",
    "    if self.form == 'sgd':\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        grad_w = layer.W_grad\n",
    "        grad_b = layer.b_grad\n",
    "\n",
    "        layer.W -= self.lr * grad_w\n",
    "        layer.b -= self.lr * grad_b\n",
    "\n",
    "    elif self.form == 'adamw':\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        grad_w = layer.W_grad\n",
    "        grad_b = layer.b_grad\n",
    "\n",
    "        #инерция\n",
    "        self.m_w[i] = self.mom1 * self.m_w[i] + (1 - self.mom1) * grad_w\n",
    "        self.v_w[i] = self.mom2 * self.v_w[i] + (1 - self.mom2) * (grad_w ** 2)\n",
    "        self.m_b[i] = self.mom1 * self.m_b[i] + (1 - self.mom1) * grad_b\n",
    "        self.v_b[i] = self.mom2 * self.v_b[i] + (1 - self.mom2) * (grad_b ** 2)\n",
    "\n",
    "        m_w_corr = self.m_w[i] / (1 - self.mom1 ** self.step_count)\n",
    "        v_w_corr = self.v_w[i] / (1 - self.mom2 ** self.step_count)\n",
    "        m_b_corr = self.m_b[i] / (1 - self.mom1 ** self.step_count)\n",
    "        v_b_corr = self.v_b[i] / (1 - self.mom2 ** self.step_count)\n",
    "\n",
    "        layer.W -= self.lr * self.wd * layer.W\n",
    "        layer.W -= self.lr * (m_w_corr / (np.sqrt(v_w_corr) + self.eps))\n",
    "\n",
    "        layer.b -= self.lr * (m_b_corr / (np.sqrt(v_b_corr) + self.eps))\n",
    "\n",
    "  def epoch(self, x, y):\n",
    "    pred = self.model.forward(x)\n",
    "    loss = self.loss_fn(pred, y)\n",
    "\n",
    "    grad = self.loss_fn.backward(pred, y)\n",
    "    self.model.backward(grad)\n",
    "    return loss\n",
    "\n",
    "  def fit(self, steps):\n",
    "    progress_bar = tqdm(range(steps))\n",
    "    for i in progress_bar:\n",
    "      loss = self.epoch(self.x, self.y)\n",
    "\n",
    "      if self.val_x is not None:\n",
    "        val_pred = self.model.forward(self.val_x)\n",
    "        val_loss = self.loss_fn(val_pred, self.val_y)\n",
    "        progress_bar.write(f\"{self.step_count}. Train loss: {loss:.6f}, validation loss: {val_loss:.6f}\")\n",
    "      else:\n",
    "        progress_bar.write(f\"{self.step_count}. Train loss: {loss:.6f}\")\n",
    "\n",
    "      self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self, layers):\n",
    "    self.layers=layers\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "  def backward(self, grad):\n",
    "    for layer in reversed(self.layers):\n",
    "      if hasattr(layer, 'backward'):\n",
    "        grad = layer.backward(grad)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYRG79HT0qae"
   },
   "source": [
    "###Validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf07127d7c54896b9ad9b807b43348f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Train loss: 1.136572, validation loss: 44.829052\n",
      "1. Train loss: 1.034230, validation loss: 48.447307\n",
      "2. Train loss: 0.938136, validation loss: 51.807335\n",
      "3. Train loss: 0.848309, validation loss: 53.104535\n",
      "4. Train loss: 0.765176, validation loss: 54.505295\n",
      "5. Train loss: 0.689312, validation loss: 55.823606\n",
      "6. Train loss: 0.623331, validation loss: 57.125582\n",
      "7. Train loss: 0.567421, validation loss: 58.512243\n",
      "8. Train loss: 0.527502, validation loss: 59.980287\n",
      "9. Train loss: 0.496438, validation loss: 61.575485\n",
      "10. Train loss: 0.468988, validation loss: 62.281105\n",
      "11. Train loss: 0.444185, validation loss: 62.302628\n",
      "12. Train loss: 0.421614, validation loss: 62.324059\n",
      "13. Train loss: 0.400970, validation loss: 62.345391\n",
      "14. Train loss: 0.381982, validation loss: 62.366616\n",
      "15. Train loss: 0.364370, validation loss: 62.387721\n",
      "16. Train loss: 0.347893, validation loss: 62.408696\n",
      "17. Train loss: 0.332356, validation loss: 62.429528\n",
      "18. Train loss: 0.317599, validation loss: 62.450205\n",
      "19. Train loss: 0.303492, validation loss: 62.470713\n",
      "20. Train loss: 0.289933, validation loss: 62.491040\n",
      "21. Train loss: 0.276847, validation loss: 62.511172\n",
      "22. Train loss: 0.264182, validation loss: 62.531096\n",
      "23. Train loss: 0.251906, validation loss: 62.550801\n",
      "24. Train loss: 0.240007, validation loss: 62.570274\n",
      "25. Train loss: 0.229335, validation loss: 62.589502\n",
      "26. Train loss: 0.219213, validation loss: 62.608479\n",
      "27. Train loss: 0.209400, validation loss: 62.627198\n",
      "28. Train loss: 0.199910, validation loss: 62.645651\n",
      "29. Train loss: 0.190775, validation loss: 62.663833\n"
     ]
    }
   ],
   "source": [
    "model = Model([\n",
    "        Linear(20, 15), ReLU(),\n",
    "        Linear(15, 10), ReLU(),\n",
    "        Linear(10, 5), ReLU(),\n",
    "        Linear(5, 2)\n",
    "])\n",
    "\n",
    "#случайные данные с разным распределением, ошибка на валидации должна оставаться\n",
    "X = np.random.rand(5,20)\n",
    "y = np.random.rand(5,2)\n",
    "X_val = np.random.rand(2,20) * 100 + 50\n",
    "y_val = np.random.rand(2,2) * 0.1 - 5\n",
    "\n",
    "opt = Optimizer(X,y,model, val=(X_val,y_val))\n",
    "opt.fit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651d35cff5464e4fadc0fd2b19240d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Train loss: 1.383958, validation loss: 1.516581\n",
      "1. Train loss: 1.362474, validation loss: 1.502840\n",
      "2. Train loss: 1.342718, validation loss: 1.489635\n",
      "3. Train loss: 1.324666, validation loss: 1.476133\n",
      "4. Train loss: 1.307909, validation loss: 1.463433\n",
      "5. Train loss: 1.292184, validation loss: 1.451298\n",
      "6. Train loss: 1.277444, validation loss: 1.439743\n",
      "7. Train loss: 1.263388, validation loss: 1.428967\n",
      "8. Train loss: 1.249910, validation loss: 1.418717\n",
      "9. Train loss: 1.237126, validation loss: 1.408589\n",
      "10. Train loss: 1.224595, validation loss: 1.399285\n",
      "11. Train loss: 1.213394, validation loss: 1.390724\n",
      "12. Train loss: 1.203499, validation loss: 1.384070\n",
      "13. Train loss: 1.194322, validation loss: 1.378504\n",
      "14. Train loss: 1.186267, validation loss: 1.375091\n"
     ]
    }
   ],
   "source": [
    "#валидация на той же функции, что и тренировка, ошибка на валидации должна опускаться\n",
    "def real_func(x):\n",
    "    return np.sin(x[:, 0:1] * 2 * np.pi) + 0.1 * np.random.randn(x.shape[0], 2)\n",
    "\n",
    "X = np.random.rand(100, 20)\n",
    "y = real_func(X)\n",
    "X_val = np.random.rand(20,20)\n",
    "y_val = real_func(X_val)\n",
    "\n",
    "opt = Optimizer(X,y,model, val=(X_val,y_val))\n",
    "opt.fit(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6564c2d0a842558805a055f3e7f8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Train loss: 1059012.679459, validation loss: 1008847.347886\n",
      "1. Train loss: 305131235.167306, validation loss: 292202523.508772\n",
      "2. Train loss: 5495440.187411, validation loss: 5260216.855171\n",
      "3. Train loss: 5.759236, validation loss: 5.886300\n",
      "4. Train loss: 8.590898, validation loss: 8.551802\n",
      "5. Train loss: 504.252582, validation loss: 491.130208\n",
      "6. Train loss: 504629.727727, validation loss: 482634.333950\n",
      "7. Train loss: 13.426520, validation loss: 13.189935\n",
      "8. Train loss: 3.947811, validation loss: 3.986313\n",
      "9. Train loss: 3.861136, validation loss: 3.898829\n",
      "10. Train loss: 3.784998, validation loss: 3.821969\n",
      "11. Train loss: 3.717715, validation loss: 3.754060\n",
      "12. Train loss: 1158.454052, validation loss: 1100.317638\n",
      "13. Train loss: 3.590584, validation loss: 3.626147\n",
      "14. Train loss: 3.557855, validation loss: 3.592603\n",
      "15. Train loss: 3.515477, validation loss: 3.549798\n",
      "16. Train loss: 3.477409, validation loss: 3.511342\n",
      "17. Train loss: 3.443144, validation loss: 3.476726\n",
      "18. Train loss: 3.412251, validation loss: 3.445514\n",
      "19. Train loss: 3.384357, validation loss: 3.417329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target[:, None]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "model = Model([\n",
    "        Linear(8, 150), ReLU(),\n",
    "        Linear(150, 50), ReLU(),\n",
    "        Linear(50, 1)\n",
    "])\n",
    "opt=Optimizer(X_train,y_train,model,val=(X_val, y_val),lr=0.1)\n",
    "opt.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc50d0221704e7db0a2c87c79a88fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Train loss: 28850.023342, validation loss: 30569.992933\n",
      "1. Train loss: 26608.318587, validation loss: 28245.809289\n",
      "2. Train loss: 24550.597916, validation loss: 26105.619468\n",
      "3. Train loss: 22043.216669, validation loss: 23492.286841\n",
      "4. Train loss: 18914.178992, validation loss: 20221.188744\n",
      "5. Train loss: 15419.609668, validation loss: 16547.808494\n",
      "6. Train loss: 11759.160966, validation loss: 12662.463660\n",
      "7. Train loss: 8393.893194, validation loss: 9020.671579\n",
      "8. Train loss: 6102.273544, validation loss: 6399.026542\n",
      "9. Train loss: 5875.959575, validation loss: 5816.776855\n",
      "10. Train loss: 6242.119917, validation loss: 6085.205222\n",
      "11. Train loss: 5585.835289, validation loss: 5612.449171\n",
      "12. Train loss: 5534.040467, validation loss: 5763.644087\n",
      "13. Train loss: 5468.631228, validation loss: 5715.549566\n",
      "14. Train loss: 5277.807576, validation loss: 5401.393754\n",
      "15. Train loss: 5228.885471, validation loss: 5324.259860\n",
      "16. Train loss: 5127.192361, validation loss: 5358.732524\n",
      "17. Train loss: 4998.416748, validation loss: 5177.631108\n",
      "18. Train loss: 4903.975187, validation loss: 5077.105323\n",
      "19. Train loss: 4842.284887, validation loss: 5120.020249\n",
      "20. Train loss: 4802.163627, validation loss: 4935.876082\n",
      "21. Train loss: 4663.288520, validation loss: 4955.313501\n",
      "22. Train loss: 4532.374479, validation loss: 4778.273289\n",
      "23. Train loss: 4466.803226, validation loss: 4667.354232\n",
      "24. Train loss: 4432.718271, validation loss: 4768.568744\n",
      "25. Train loss: 4311.692833, validation loss: 4517.136189\n",
      "26. Train loss: 4192.154984, validation loss: 4482.001354\n",
      "27. Train loss: 4095.735168, validation loss: 4373.101504\n",
      "28. Train loss: 4001.539705, validation loss: 4263.505283\n",
      "29. Train loss: 3958.274385, validation loss: 4307.630184\n",
      "30. Train loss: 3998.723515, validation loss: 4181.387956\n",
      "31. Train loss: 3817.467958, validation loss: 4153.438913\n",
      "32. Train loss: 3725.570975, validation loss: 4035.370118\n",
      "33. Train loss: 3725.433312, validation loss: 3933.909809\n",
      "34. Train loss: 3654.299323, validation loss: 3981.314798\n",
      "35. Train loss: 3540.854696, validation loss: 3784.629828\n",
      "36. Train loss: 3475.827510, validation loss: 3721.442882\n",
      "37. Train loss: 3436.941175, validation loss: 3716.693061\n",
      "38. Train loss: 3538.060748, validation loss: 3691.098030\n",
      "39. Train loss: 3448.046615, validation loss: 3742.290732\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target[:, None]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "model = Model([\n",
    "        Linear(10, 150), ReLU(),\n",
    "        Linear(150, 100), ReLU(),\n",
    "        Linear(100, 50), ReLU(),\n",
    "        Linear(50, 1)\n",
    "])\n",
    "opt=Optimizer(X_train,y_train,model,lr=0.01,val=(X_val,y_val),betas=(0.5,0.5))\n",
    "opt.fit(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8i8Pvt82t07"
   },
   "source": [
    "###Concat, SumM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      " [[ 1  2  3 10]\n",
      " [ 4  5  6 11]\n",
      " [ 7  8  9 12]]\n"
     ]
    }
   ],
   "source": [
    "# Concat\n",
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = np.array([[10, 11, 12]])\n",
    "c1, c2 = Concat(0), Concat(1)\n",
    "print(c1(a,b),'\\n\\n',c2(a,b.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.7, 0.3, 0.6],\n",
      "       [0.4, 0.3, 0.7],\n",
      "       [0.3, 0.5, 0.9]]), array([[0.9, 1. , 0.3]])] \n",
      "\n",
      " [array([[0.5, 0.2, 0.3],\n",
      "       [0.6, 0.2, 0.3],\n",
      "       [1. , 0. , 0.9]]), array([[0.4],\n",
      "       [0.9],\n",
      "       [0.2]])]\n"
     ]
    }
   ],
   "source": [
    "print(c1.backward(np.random.rand(4, 3).round(1)), '\\n\\n', c2.backward(np.random.rand(3, 4).round(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 13, 15],\n",
       "       [14, 16, 18],\n",
       "       [17, 19, 21]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SumM\n",
    "s = SumM()\n",
    "s(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.8, 0.5, 0.3],\n",
       "        [0.3, 0.4, 0.2],\n",
       "        [0.5, 0.7, 0.5]]),\n",
       " array([[0.8, 0.5, 0.3],\n",
       "        [0.3, 0.4, 0.2],\n",
       "        [0.5, 0.7, 0.5]])]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.backward(np.random.rand(3,3).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb7zsqTjnkHe"
   },
   "source": [
    "##Not pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class mult:\n",
    "  \"Слой перемножения матриц: y = X @ W\"\n",
    "  def __init__(self,w,x):\n",
    "    self.w,self.x=w,x\n",
    "  def __call__(self):\n",
    "    self.res=self.x@self.w\n",
    "    return self.res\n",
    "  def backward(self,grad):\n",
    "    self.w_grad=self.x.T@grad\n",
    "    self.x_grad=grad@self.w.T\n",
    "    return self.x_grad\n",
    "\n",
    "class addC:\n",
    "  \"Добавление константы (смещения)\"\n",
    "  def __init__(self,b):\n",
    "    self.b=b\n",
    "  def __call__(self,res):\n",
    "    self.res=res+self.b\n",
    "    return self.res\n",
    "  def backward(self,grad):\n",
    "    self.b_grad=np.sum(grad,axis=0)\n",
    "    return grad\n",
    "\n",
    "class LL:\n",
    "  \"Линейный слой\"\n",
    "  def __init__(self,w,b):\n",
    "    self.w,self.b=w*np.sqrt(2/w.shape[0]),b\n",
    "    self.grad=None\n",
    "    self.l,self.x=None,None\n",
    "  def forward(self,l):\n",
    "    if isinstance(l,LL):\n",
    "      self.l=l\n",
    "      self.x=self.l.y\n",
    "    else:\n",
    "      self.x=l\n",
    "    self.mult=mult(self.w,self.x)\n",
    "    self.add=addC(self.b)\n",
    "    self.y=self.add(self.mult())\n",
    "    return self\n",
    "  def backward(self,grad):\n",
    "    grad=self.add.backward(grad)\n",
    "    grad=self.mult.backward(grad)\n",
    "    self.w_grad=self.mult.w_grad\n",
    "    self.b_grad=self.add.b_grad\n",
    "    #print(f'LAYER w grad: {self.wgrad}, b grad: {self.bgrad}')\n",
    "    return grad\n",
    "\n",
    "class relu:\n",
    "  \"Нелинейность, ReLU\"\n",
    "  def __init__(self):\n",
    "    self.grad,self.x=None,None\n",
    "  def forward(self,x):\n",
    "    self.x=x\n",
    "    self.y=np.clip(self.x,0,np.inf)\n",
    "    return self\n",
    "  def backward(self,grad):\n",
    "    self.grad=(self.x > 0).astype(float)\n",
    "    #print(f'RELU grad: {self.grad}')\n",
    "    return grad*self.grad\n",
    "\n",
    "class mse:\n",
    "  \"Ошибка по формуле MSE\"\n",
    "  def __init__(self,ytrue):\n",
    "    self.ytrue=ytrue\n",
    "    self.grad=None\n",
    "  def forward(self,ypred):\n",
    "    self.ypred=ypred\n",
    "    self.n=len(self.ytrue)\n",
    "    self.sq_er=(self.ypred-self.ytrue)**2\n",
    "    self.sum_ll=sumE()\n",
    "    self.loss=self.sum_ll(self.sq_er)/self.n\n",
    "    return self\n",
    "  def backward(self):\n",
    "    self.grad=2*(self.ypred-self.ytrue)/self.n\n",
    "    #print(f'MSE grad: {self.grad}')\n",
    "    return self.grad[None,:]\n",
    "\n",
    "class sumE:\n",
    "  \"Сложение элементов одной матрицы\"\n",
    "  def __init__(self):\n",
    "    self.shape=None\n",
    "  def forward(self,x):\n",
    "    self.shape=x.shape\n",
    "    self.y=x.sum()\n",
    "    return self.y\n",
    "  def __call__(self,x):\n",
    "    return self.forward(x)\n",
    "  def backward(self,grad):\n",
    "    self.grad=np.ones(self.shape)\n",
    "    return self.grad*grad\n",
    "\n",
    "class sumM:\n",
    "  \"Сложение матриц\"\n",
    "  def __init__(self):\n",
    "    self.shapes=None\n",
    "  def forward(self,tensors):\n",
    "    self.shapes=[t.shape for t in tensors]\n",
    "    self.y=tensors[0]\n",
    "    for t in tensors[1:]:\n",
    "      self.y+=t\n",
    "    return self\n",
    "  def backward(self,grad):\n",
    "    return [grad]*len(self.shapes)\n",
    "\n",
    "class concat:\n",
    "  \"Конкатенация тензоров\"\n",
    "  def __init__(self,axis):\n",
    "    self.shapes=None\n",
    "    self.axis=axis\n",
    "  def forward(self,*tensors):\n",
    "    self.shapes=[t.shape for t in tensors]\n",
    "    self.y=np.concatenate(tensors,axis=self.axis)\n",
    "    return self\n",
    "  def __call__(self,*tensors):\n",
    "    return self.forward(*tensors)\n",
    "  def backward(self,grad):\n",
    "    grads=[]\n",
    "    start_idx=0\n",
    "    for shape in self.shapes:\n",
    "      end_idx=start_idx+shape[self.axis]\n",
    "      slices=[]\n",
    "      for dim in range(grad.ndim):\n",
    "        if dim==self.axis:\n",
    "          slices.append(slice(start_idx,end_idx))\n",
    "        else:\n",
    "          slices.append(slice(None))\n",
    "      grads.append(grad[tuple(slices)])\n",
    "      start_idx=end_idx\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "  def __init__(self,x,y,layers,lr=0.001,type='adamw',betas=(0.9,0.99),wd=0.001,eps=10**(-8)):\n",
    "    self.x,self.y=x,y\n",
    "    self.type=type.lower()\n",
    "    self.layers=layers\n",
    "    self.denses=layers.getLLs()\n",
    "    self.lr,self.wd=lr,wd\n",
    "    self.mom1,self.mom2=betas\n",
    "    self.eps=eps\n",
    "    self.params=layers.params()\n",
    "    self.step_count=0\n",
    "    self.m=[np.zeros_like(p) for p in self.params]\n",
    "    self.v=[np.zeros_like(p) for p in self.params]\n",
    "  def epoch(self,x,y,i):\n",
    "    for layer in self.layers.lls:\n",
    "        x=layer.forward(x).y\n",
    "\n",
    "    self.loss=mse(y).forward(x)\n",
    "    print(f'{i+1}. loss:',self.loss.loss)\n",
    "\n",
    "    self.grad=self.loss.backward()\n",
    "    for layer in reversed(self.layers.lls):\n",
    "      self.grad=layer.backward(self.grad)\n",
    "\n",
    "  def step(self):\n",
    "    self.step_count+=1\n",
    "    if self.type=='sgd':\n",
    "      for i, ll in enumerate(self.denses):\n",
    "        grad_mean_w=ll.w_grad.mean(axis=0)\n",
    "        grad_mean_b=ll.b_grad.mean(axis=0)\n",
    "        ll.w-=self.lr*grad_mean_w\n",
    "        ll.b-=self.lr*grad_mean_b\n",
    "    elif self.type=='adamw':\n",
    "      for i, ll in enumerate(self.denses):\n",
    "        grad_mean_w=ll.w_grad.mean(axis=0)\n",
    "        self.m[i]=self.mom1*self.m[i]+(1-self.mom1)*grad_mean_w\n",
    "        self.v[i]=self.mom2*self.v[i]+(1-self.mom2)*(grad_mean_w**2)\n",
    "        m_corr=self.m[i]/(1-self.mom1**self.step_count)\n",
    "        v_corr=self.v[i]/(1-self.mom2**self.step_count)\n",
    "        ll.w-=self.lr*(m_corr / (np.sqrt(v_corr) + self.eps) + self.wd*ll.w)\n",
    "\n",
    "        grad_mean_b=ll.b_grad.mean(axis=0)\n",
    "        ll.b-=self.lr*grad_mean_b\n",
    "  def fit(self,steps):\n",
    "    for i in range(steps):\n",
    "      self.epoch(self.x,self.y,i)\n",
    "      self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "  def __init__(self,lls):\n",
    "    self.lls=lls\n",
    "    self.parameters=[]\n",
    "  def getLLs(self):\n",
    "    self.dense_lls=[]\n",
    "    for ll in self.lls:\n",
    "      if type(ll)==LL:\n",
    "        self.dense_lls.append(ll)\n",
    "    return self.dense_lls\n",
    "  def params(self):\n",
    "    self.parameters=[]\n",
    "    for ll in self.lls:\n",
    "      if type(ll)==LL:\n",
    "        self.parameters.append(ll.w)\n",
    "    return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loss: 479.7989441964023\n",
      "2. loss: 0.22695112268683335\n",
      "3. loss: 0.5507728725686756\n",
      "4. loss: 0.6964999874120906\n"
     ]
    }
   ],
   "source": [
    "ll1=LL(np.random.rand(20,15),np.random.rand(15))\n",
    "ll2=relu()\n",
    "ll3=LL(np.random.rand(15,10),np.random.rand(10))\n",
    "ll4=relu()\n",
    "ll5=LL(np.random.rand(10,5),np.random.rand(5))\n",
    "ll6=relu()\n",
    "ll7=LL(np.random.rand(5,1),np.random.rand(1))\n",
    "layers=[ll1,ll2,ll3,ll4,ll5,ll6,ll7]\n",
    "layers=Layers(layers)\n",
    "x,y=np.random.rand(5,20),np.random.rand(5).reshape(-1,1)\n",
    "opt=Optimizer(x,y,layers,lr=0.1,betas=(0.9,0.99))\n",
    "opt.fit(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9l24NcpyBqD"
   },
   "source": [
    "###Iris classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (150, 4)\n",
      "Target (y) shape: (150,)\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Access features (X) and target (y)\n",
    "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
    "y = iris.target # Target (species: 0 for setosa, 1 for versicolor, 2 for virginica)\n",
    "\n",
    "# Access feature names and target names\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loss: 819454.0391322155\n",
      "2. loss: 1590.162177188977\n",
      "3. loss: 93.03753718906862\n",
      "4. loss: 6.8514885148656735\n",
      "5. loss: 2.076232445048056\n",
      "6. loss: 2.8053036626956978\n",
      "7. loss: 3.5277286962525576\n",
      "8. loss: 3.998147827762829\n",
      "9. loss: 4.298224286985275\n",
      "10. loss: 4.489563474382215\n",
      "11. loss: 4.622065489470174\n",
      "12. loss: 4.71481285108289\n",
      "13. loss: 4.779711645410926\n",
      "14. loss: 4.826985584508983\n",
      "15. loss: 4.859029159165757\n",
      "16. loss: 4.881541040250126\n",
      "17. loss: 4.895723989499301\n",
      "18. loss: 4.902883091403396\n",
      "19. loss: 4.906568804543523\n",
      "20. loss: 4.907037143144102\n",
      "21. loss: 4.903682464937312\n",
      "22. loss: 4.896529746267512\n",
      "23. loss: 4.88660409162969\n",
      "24. loss: 4.873331661348823\n",
      "25. loss: 4.85634394473958\n",
      "26. loss: 4.834620234537198\n",
      "27. loss: 4.805915241973571\n",
      "28. loss: 4.766364689604884\n",
      "29. loss: 4.701626669165847\n",
      "30. loss: 4.579219579674438\n",
      "31. loss: 4.2447794608406335\n",
      "32. loss: 3.3404093742031575\n",
      "33. loss: 1.5476209543377437\n",
      "34. loss: 0.5567548623235404\n",
      "35. loss: 0.499567036567186\n",
      "36. loss: 0.6569693226592085\n",
      "37. loss: 0.28523848744079683\n",
      "38. loss: 0.6858362365157809\n",
      "39. loss: 0.647376994175843\n",
      "40. loss: 0.42766046791535267\n",
      "41. loss: 0.4128752332889785\n",
      "42. loss: 0.27385614252639195\n",
      "43. loss: 0.17030398136316918\n",
      "44. loss: 0.33986197646880695\n",
      "45. loss: 0.5915065827604458\n",
      "46. loss: 0.13721905437736073\n",
      "47. loss: 1.055008694672479\n",
      "48. loss: 0.553458339145485\n",
      "49. loss: 0.763362288041568\n",
      "50. loss: 0.12940615365621203\n"
     ]
    }
   ],
   "source": [
    "ll1=LL(np.random.rand(4,15),np.random.rand(15))\n",
    "ll2=relu()\n",
    "ll3=LL(np.random.rand(15,10),np.random.rand(10))\n",
    "ll4=relu()\n",
    "ll5=LL(np.random.rand(10,5),np.random.rand(5))\n",
    "ll6=relu()\n",
    "ll7=LL(np.random.rand(5,1),np.random.rand(1))\n",
    "layers=[ll1,ll2,ll3,ll4,ll5,ll6,ll7]\n",
    "layers=Layers(layers)\n",
    "opt=Optimizer(X,y,layers,lr=0.1,betas=(0.5,0.5))\n",
    "opt.fit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.09704974280592289),\n",
       " np.float64(0.9624718185878698),\n",
       " np.float64(1.7032070809668485))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll7.y[:50].mean(),ll7.y[50:100].mean(),ll7.y[100:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3F1MqkF0CEw"
   },
   "source": [
    "###Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Access features and target\n",
    "X = housing.data  # Features\n",
    "y = housing.target[:,None]  # Target (median house value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loss: 20125856734.039833\n",
      "2. loss: 7511471032.612467\n",
      "3. loss: 2631984486.001109\n",
      "4. loss: 878349412.7770873\n",
      "5. loss: 276030781.8890592\n",
      "6. loss: 78761380.25008124\n",
      "7. loss: 18965288.410181224\n",
      "8. loss: 3277109.2206771155\n",
      "9. loss: 276219.62045823276\n",
      "10. loss: 6448.968111263429\n",
      "11. loss: 144.06329621914762\n",
      "12. loss: 10.145576309419875\n",
      "13. loss: 2.015314044121069\n",
      "14. loss: 2.1112561333893787\n",
      "15. loss: 2.2060471999775104\n",
      "16. loss: 2.297279697382872\n",
      "17. loss: 2.3843438864131365\n",
      "18. loss: 2.4670683569830794\n",
      "19. loss: 2.5452726579384026\n",
      "20. loss: 2.6189822083361918\n",
      "21. loss: 2.68814855965768\n",
      "22. loss: 2.7528260351475904\n",
      "23. loss: 2.813175095290644\n",
      "24. loss: 2.8693582084340776\n",
      "25. loss: 2.92157476541411\n",
      "26. loss: 2.9700206760535677\n",
      "27. loss: 3.0148964164700436\n",
      "28. loss: 3.056408784857749\n",
      "29. loss: 3.0947656496905465\n",
      "30. loss: 3.130165677696078\n",
      "31. loss: 3.1628049977557633\n",
      "32. loss: 3.192870004118795\n",
      "33. loss: 3.2205401704327166\n",
      "34. loss: 3.245984678068353\n",
      "35. loss: 3.269365310043478\n",
      "36. loss: 3.2908341622190544\n",
      "37. loss: 3.3105351209077587\n",
      "38. loss: 3.3286029475658423\n",
      "39. loss: 3.345163455063076\n",
      "40. loss: 3.36033445102764\n"
     ]
    }
   ],
   "source": [
    "ll1=LL(np.random.rand(8,150),np.random.rand(150))\n",
    "ll2=relu()\n",
    "ll3=LL(np.random.rand(150,100),np.random.rand(100))\n",
    "ll4=relu()\n",
    "ll5=LL(np.random.rand(100,50),np.random.rand(50))\n",
    "ll6=relu()\n",
    "ll7=LL(np.random.rand(50,1),np.random.rand(1))\n",
    "layers=[ll1,ll2,ll3,ll4,ll5,ll6,ll7]\n",
    "layers=Layers(layers)\n",
    "opt=Optimizer(X,y,layers,lr=0.01,betas=(0.9,0.99))\n",
    "opt.fit(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkHp7bgH2lcd"
   },
   "source": [
    "idk if it works but numbers do go down"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
